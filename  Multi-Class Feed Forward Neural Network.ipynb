{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ffnn model self multi class.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keval47/Machine-Learning/blob/master/%20Multi-Class%20Feed%20Forward%20Neural%20Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFTtZ_Sg8T18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGlzoruW86XM",
        "colab_type": "code",
        "outputId": "ab974ab1-e388-442a-abca-4d76ac7a90e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "data = datasets.load_iris()\n",
        "X = data.data\n",
        "Y = data.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y,random_state = 5)\n",
        "\n",
        "y_train = y_train.reshape(len(y_train),1)\n",
        "y_test = y_test.reshape(len(y_test),1)\n",
        "\n",
        "\n",
        "print(X_train.shape,y_train.shape)"
      ],
      "execution_count": 384,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(112, 4) (112, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grydxrvR3-qx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def OneHotEncoder(x):\n",
        "    arr = np.zeros((len(x),np.max(x)+1),dtype=int)\n",
        "    for i,val in enumerate(x):\n",
        "        arr[i][val[0]] = 1\n",
        "    return arr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqYVVaxX6i6X",
        "colab_type": "code",
        "outputId": "bb06b5e9-8efd-453e-9e9b-f7823263afe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_train = OneHotEncoder(y_train)\n",
        "print(X_train.shape,y_train.shape)"
      ],
      "execution_count": 386,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(112, 4) (112, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nStkLOwg-wNy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Neuron:\n",
        "    def __init__(self,inputNeuron,hiddenNeuron,outputNeuron):\n",
        "        self.w_ih = np.random.random((inputNeuron,hiddenNeuron)) # (2x3) weights b/w input and hidden layer         \n",
        "        self.b_ih = np.random.random((1,hiddenNeuron)) #(1x3) bias b/w input and hidden layer\n",
        "\n",
        "        self.w_ho = np.random.random((hiddenNeuron,outputNeuron)) # (3x1) weights b/w hidden and output layer\n",
        "        self.b_ho = np.random.random((1,outputNeuron)) #(1x1) bias b/w  hidden and output layer\n",
        "\n",
        "        self.hiddenLayerOutput = np.array((hiddenNeuron,1),dtype=np.float64) # (3x1) just for store output generated by model\n",
        "        self.outputLayerOutput = np.array((outputNeuron,1),dtype=np.float64) # (1x1) just for store output generated by model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYhf5i7-8nru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ffnn:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def layers(self,inputNeuron,hiddenNeuron,outputNeuron):\n",
        "        self._neuron = Neuron(inputNeuron,hiddenNeuron,outputNeuron)# creating neurons\n",
        "\n",
        "    def hyper_params(self,lr=0.5,epochs=1000):\n",
        "        self._lr = lr # learning rate\n",
        "        self._epochs = epochs #iteration how many times data will be train\n",
        "\n",
        "    def fit(self,X,Y):\n",
        "        self._x = X\n",
        "        self._y = Y\n",
        "\n",
        "    # function for matrix multiplication you can also use np.dot\n",
        "    def _dot(self,a,b):\n",
        "        matmul = []\n",
        "        for val in a:\n",
        "            valSum = 0\n",
        "            for i in range(len(val)):\n",
        "                valSum += val[i] * b[i]\n",
        "            matmul.append(valSum)\n",
        "        return np.array(matmul)\n",
        "    \n",
        "    # squzze value in between 1/0. return probability\n",
        "    def _sigmoid(self,z,deriv=False):\n",
        "        if(deriv):\n",
        "            return z * (1-z)\n",
        "        return 1/(1+(np.exp(-z)))\n",
        "    \n",
        "    def _softmax(self,x):\n",
        "\n",
        "        # unstable\n",
        "        # x = x-x.max()\n",
        "        # expA = np.exp(x)         \n",
        "        # return expA / expA.sum()   \n",
        "\n",
        "        #stable\n",
        "        xrel = x - x.max(axis=1, keepdims=True)# make every value 0 or below, as exp(0) won't overflow\n",
        "        exp_xrel = np.exp(xrel)\n",
        "        return exp_xrel / exp_xrel.sum(axis=1, keepdims=True) \n",
        "    \n",
        "    # return weighted sum. return output based on weights\n",
        "    def _feedforward(self):\n",
        "        self._neuron.hiddenLayerOutput = self._sigmoid(self._dot(self._x,self._neuron.w_ih)+self._neuron.b_ih)\n",
        "        self._neuron.outputLayerOutput = self._softmax(self._dot(self._neuron.hiddenLayerOutput,self._neuron.w_ho)+self._neuron.b_ho)\n",
        "\n",
        "    # perform backpropagation. adjusts weights that let us close to accurate output \n",
        "    def _backProp(self):\n",
        "        # print(self._sigmoid(self._neuron.outputLayerOutput))\n",
        "        # print(\"\\n\")\n",
        "        # print(self._sigmoid(self._neuron.outputLayerOutput,True))\n",
        "        # return\n",
        "        err_ho = self._y - self._neuron.outputLayerOutput #calculate gradient of err. gradient=direction\n",
        "        slope_ho = self._sigmoid(self._neuron.outputLayerOutput,True) # calculate slope of output\n",
        "        delta_ho = err_ho * slope_ho #change in w_ho weights. define direction that make less error\n",
        "\n",
        "        err_ih = self._dot(delta_ho,self._neuron.w_ho.T)  # define gradient of err based on output layer error\n",
        "        slope_ih = self._sigmoid(self._neuron.hiddenLayerOutput,True) #calculate slope of hidden layer output\n",
        "        delta_ih = err_ih * slope_ih #change in i_ho weights. define direction that make less error\n",
        "        \n",
        "        # adjust weights to make less err\n",
        "        self._neuron.w_ih += self._dot(self._x.T,delta_ih) * self._lr #step further to defined direction by delta.step = learning rate\n",
        "        self._neuron.w_ho += self._dot(self._neuron.hiddenLayerOutput.T,delta_ho) * self._lr #step further to defined direction by delta. step = learning rate\n",
        "\n",
        "         # adjust bias to make less err\n",
        "        self._neuron.b_ih += np.sum(delta_ih,axis=0) * self._lr #step further to defined direction by delta. step = learning rate\n",
        "        self._neuron.b_ho += np.sum(delta_ho,axis=0) * self._lr #step further to defined direction by delta. step = learning rate\n",
        "\n",
        "    def _err(self):\n",
        "        # mean squared err\n",
        "        err = np.mean(np.square(self._y - self._neuron.outputLayerOutput),dtype=np.float64)\n",
        "        return err\n",
        "\n",
        "    def predict(self,val):\n",
        "        # predict probability\n",
        "        # if you want to predict class then simply put threshold and filtered out class\n",
        "        ho = self._sigmoid(self._dot(val,self._neuron.w_ih)+self._neuron.b_ih)\n",
        "        return self._softmax(self._dot(ho,self._neuron.w_ho)+self._neuron.b_ho)\n",
        "        # return self._sigmoid(self._dot(val,self._neuron.w_ho)+self._neuron.b_ho)\n",
        "\n",
        "    def train(self,printErr=False):\n",
        "        # itrating over and over to make model train\n",
        "        for i in range(self._epochs):\n",
        "            self._feedforward()\n",
        "            self._backProp()\n",
        "            if(printErr):\n",
        "                # print err by every 100 iteration\n",
        "                if(i % 100 == 0):\n",
        "                    print(self._err())\n",
        "        return \"training finished\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGa14MxS8800",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# running model\n",
        "neuralnet = ffnn()\n",
        "neuralnet.hyper_params(lr=0.6,epochs=1000)\n",
        "neuralnet.layers(4,4,3)\n",
        "neuralnet.fit(X_train,y_train)\n",
        "neuralnet.train(printErr=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZDF0BL8BGxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicts = np.zeros((len(X_test),1),dtype=int)\n",
        "for x in range(len(X_test)):\n",
        "    p =  neuralnet.predict([X_test[x]])\n",
        "    predicts[x] = np.argmax(p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojosNaTaFY8g",
        "colab_type": "code",
        "outputId": "0d61820b-5296-42da-8336-66316e87dece",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "accuracy_score(y_test,predicts)"
      ],
      "execution_count": 391,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9473684210526315"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 391
        }
      ]
    }
  ]
}